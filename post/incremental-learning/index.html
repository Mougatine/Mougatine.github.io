<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.45.1" />
  <meta name="author" content="Arthur Douillard">

  
  
  
  
    
      
    
  
  <meta name="description" content="Learning tasks incrementally using knowledge distillation &amp; external memory.">

  
  <link rel="alternate" hreflang="en-us" href="/post/incremental-learning/">

  


  

  
  
  
  <meta name="theme-color" content="hsl(339, 90%, 68%)">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    

    

    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/instantsearch.js/2.9.0/instantsearch.min.css" integrity="sha256-ZtmLe16p4jS4/2wPwwH6NzJt460SJzgLmhKrYo5yn7g=" crossorigin="anonymous">
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/instantsearch.js/2.9.0/instantsearch-theme-algolia.min.css" integrity="sha256-0vcZrdMQksHcHrY60tPnVK71jnB3wX/JpMcah5BffjA=" crossorigin="anonymous">
    

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700%7cRoboto:400,400italic,700%7cRoboto&#43;Mono">
  

  <link rel="stylesheet" href="/styles.css">
  
  <link rel="stylesheet" href="/css/search_bar.css">
  

  
  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-102629480-1', 'auto');
      
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="//www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  
  

  
  <link rel="alternate" href="/index.xml" type="application/rss+xml" title="Arthur Douillard">
  <link rel="feed" href="/index.xml" type="application/rss+xml" title="Arthur Douillard">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="/post/incremental-learning/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@Ar_Douillard">
  <meta property="twitter:creator" content="@Ar_Douillard">
  
  <meta property="og:site_name" content="Arthur Douillard">
  <meta property="og:url" content="/post/incremental-learning/">
  <meta property="og:title" content="Introduction to Incremental Learning | Arthur Douillard">
  <meta property="og:description" content="Learning tasks incrementally using knowledge distillation &amp; external memory.">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2019-04-26T00:00:00&#43;02:00">
  
  <meta property="article:modified_time" content="2019-04-26T00:00:00&#43;02:00">
  

  

  
<meta name="twitter:title" content="Introduction to Incremental Learning" />



<meta name="twitter:description" content="Learning tasks incrementally using knowledge distillation &amp; external memory." />







<meta property="og:description" content="Learning tasks incrementally using knowledge distillation &amp; external memory."/>


<link rel="icon" type="image/png" href="/img/favicon32.png">
<link rel="apple-touch-icon" type="image/png" href="/img/favicon180.png">

  <title>Introduction to Incremental Learning | Arthur Douillard</title>

</head>
<body id="top" data-spy="scroll" data-target="#toc" data-offset="71" class="dark">

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
      <a class="navbar-brand" href="/">Arthur Douillard</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      
      <ul class="nav navbar-nav navbar-right">
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#posts">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#talks_list">
            
            <span>Talks</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#teaching">
            
            <span>Teaching</span>
            
          </a>
        </li>

        
        
      

      
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  


  <div class="article-container">
    <h1 itemprop="name">Introduction to Incremental Learning</h1>

    

<div class="article-metadata">

  
  
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Arthur Douillard">
  </span>
  

  <span class="article-date">
    
    <meta content="2019-04-26 00:00:00 &#43;0200 CEST" itemprop="datePublished">
    <time datetime="2019-04-26 00:00:00 &#43;0200 CEST" itemprop="dateModified">
      26 April, 2019
    </time>
  </span>
  <span itemscope itemprop="publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Arthur Douillard">
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    6 min read
  </span>
  

  
  

  

  
  
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=Introduction%20to%20Incremental%20Learning&amp;url=%2fpost%2fincremental-learning%2f"
         target="_blank" rel="noopener">
        <i class="fa fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=%2fpost%2fincremental-learning%2f"
         target="_blank" rel="noopener">
        <i class="fa fa-facebook"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=%2fpost%2fincremental-learning%2f&amp;title=Introduction%20to%20Incremental%20Learning"
         target="_blank" rel="noopener">
        <i class="fa fa-linkedin"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=%2fpost%2fincremental-learning%2f&amp;title=Introduction%20to%20Incremental%20Learning"
         target="_blank" rel="noopener">
        <i class="fa fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=Introduction%20to%20Incremental%20Learning&amp;body=%2fpost%2fincremental-learning%2f">
        <i class="fa fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


  

</div>


    <div class="article-style" itemprop="articleBody">
      

<h2 id="transfer-learning">Transfer Learning</h2>

<p><strong>Transfer learning</strong> allows to transfer the knowledge gained on one task (e.g
<em>ImageNet</em>) to another task (e.g <em>classify cats &amp; dogs</em>). Usually the backbone
(a ConvNet in Computer Vision, like ResNet) is kept while a new classifier is
plugged in on top of it. During transfer, we train the new classifier &amp;
<strong>fine-tune</strong> the backbone.</p>

<p>However by fine-tuning the model&rsquo;s backbone on the new task, it will probably change
it so much that it will forget the old task!</p>

<p>Neural Networks are good to learn, but bad to retain knowledge from previous task.</p>

<p>This is called a <strong>catastrophic forgetting</strong> <a href="https://www.sciencedirect.com/science/article/pii/S1364661399012942" target="_blank">(French, 1995)</a>. To solve it, one must be an optimal trade-off between
<strong>rigidity</strong> (being good on old classes) and <strong>plasticity</strong> (being good on new classes).</p>

<h2 id="lifelong-learning">Lifelong-Learning</h2>

<p>This lack of knowledge retention is a problem in two cases:</p>

<ul>
<li>(Ideal) Robots must learn continuously without forgetting
-. New tasks (or new samples) keep coming but we don&rsquo;t want:

<ul>
<li>to store the increasingly bigger dataset</li>
<li>to retrain on all the data in order to not forget anything</li>
</ul></li>
</ul>

<p>The goal of <strong>lifelong-learning</strong> is thus three folds:</p>

<ul>
<li>Learn new tasks or new samples without forgetting the previous ones</li>
<li>The model is kept between tasks in order to make us of the incremental knowledge</li>
<li>All the seen data cannot be stored, either keep nothing or a constant-size subset.</li>
</ul>

<p>Lifelong-learning can also be divided in three scenarios
<a href="https://arxiv.org/abs/1705.03550" target="_blank">(Lomonaco and Maltoni, 2017)</a>:</p>

<ul>
<li><strong>New instances</strong> added with potentially new domains (<em>online learning</em>)</li>
<li><strong>New classes</strong> added (<em>incremental learning</em>)</li>
<li><strong>New instances &amp; new classes</strong> added</li>
</ul>

<p>In this article we will focus only on the incremental aspect. Note however that the
methods used are fairly similar between scenarios.</p>

<h2 id="small-literature-review">Small literature review</h2>

<p><a href="https://arxiv.org/abs/1802.07569" target="_blank">(Parisi et al, 2018)</a> defines 3 broad strategies:</p>

<ul>
<li><strong>External Memory</strong> storing previous tasks data</li>
<li><strong>Regularization</strong> methods avoiding forgetting on previous tasks</li>
<li><strong>Model Plasticity</strong> extending the capacity</li>
</ul>

<h4 id="external-memory">External Memory</h4>

<p>Lifelong-learning has for constraint to have a size-bounded training dataset. The
role of the external memory is thus to sample only a subset of every old tasks.</p>

<p>Three variants exists:</p>

<ul>
<li><strong>Reharsal learning</strong> keeps a subsample of previous data</li>
<li><strong>Pseudo-Reharsal Learning</strong> generates data using previous data&rsquo;s distribution</li>
<li><strong>No memory</strong>, training only on new data</li>
</ul>

<h4 id="regularization">Regularization</h4>

<p>Regularization&rsquo;s goal in lifelong-learning is to force the model trained on task $T + 1$
to be similar enough to its previous version trained on task $T$. So that the
catastrophic forgetting is alleviated.</p>

<p>There are several ways to do so:</p>

<ul>
<li>The most obvious is to add to the loss a L2 distance between the weights of the
old &amp; new version of the model. <a href="https://arxiv.org/abs/1612.00796" target="_blank">(Kirkpatrick et al, 2017)</a>
and <a href="https://arxiv.org/abs/1703.04200" target="_blank">(Zenke, Poole, and Ganguli, 2017)</a> add an
<strong>importance factor</strong> to make this distance more or less &ldquo;elastic&rdquo;. An important
weight would be allowed to change less than a lesser important weight.</li>
<li><a href="https://arxiv.org/abs/1606.09282" target="_blank">(Li and Hoiem, 2018)</a>, <a href="https://arxiv.org/abs/1611.07725" target="_blank">(Rebuffi et al, 2017)</a>, and <a href="https://arxiv.org/abs/1807.09536" target="_blank">(Castro et al, 2018)</a> add
a constraint on the probabilities. If the previous model attributed a confidence of $56%$
on the class $C$ for the sample $S$ then the new model should attribute it a close confidence.</li>
<li>Finally <a href="https://arxiv.org/abs/1706.08840" target="_blank">(Lopez-Paz and Ranzato, 2017)</a> imposes
a constraint on the loss itself by forbidding to increase on seen samples.</li>
</ul>

<h4 id="model-plasticity">Model plasticity</h4>

<p>Most lifelong-learning algorithms add new neurons to their classifier in order
to predict the new classes. <a href="https://arxiv.org/abs/1903.04476" target="_blank">(Golkar, Kagan and Cho, 2019)</a>
doesn&rsquo;t and instead share the same classifier for every task, each task uses
a mask choosing the input neurons.</p>

<p><a href="https://arxiv.org/abs/1708.01547" target="_blank">(Yoon et al, 2018)</a> rationalizes that the total
model capacity can not suffice sometimes with the increasing number of tasks. Its
Dynamically Expandable Networks can copy existing neurons or even add new neurons
to solve this.</p>

<p><a href="https://arxiv.org/abs/1803.03635" target="_blank">(Frankle and Carbin, 2018)</a>&rsquo;s Lottery Ticket
Hypothesis states that neural networks are vastly overparametrized &amp; that we
can prune them down to a small sub-network having similar performance.
<a href="https://arxiv.org/abs/1701.08734" target="_blank">(Fernando et al, 2017)</a>&rsquo;s PathNet explores those
sub-networks using an evolutionary algorithms &amp; uses one sub-network per task.
<a href="https://arxiv.org/abs/1903.04476" target="_blank">(Golkar, Kagan and Cho, 2019)</a> imitates PathNet
but instead find the sub-networks using an in-training neurons sparsification
with a L1 regularization on the neurons activity (<a href="https://stats.stackexchange.com/a/159379" target="_blank">see intuition</a>).</p>

<h2 id="our-focus">Our focus</h2>

<p>I&rsquo;ll detail now three papers, each an incremental (see the pun?) improvement over the
precedent:</p>

<ul>
<li>Learning without Forgetting <a href="https://arxiv.org/abs/1606.09282" target="_blank">(Li and Hoiem, 2016)</a>.</li>
<li>iCaRL: Incremental Classifier and Representation Learning <a href="https://arxiv.org/abs/1611.07725" target="_blank">(Rebuffi et al, 2017)</a></li>
<li>End-to-End Incremental Learning <a href="https://arxiv.org/abs/1807.09536" target="_blank">(Castro et al, 2018)</a></li>
</ul>

<p>You can find my PyTorch implementations <a href="https://github.com/arthurdouillard/incremental_learning.pytorch" target="_blank">here</a>.</p>

<h4 id="evaluation">Evaluation</h4>

<p>The three papers are evaluated on the iCIFAR100 benchmark: a training on CIFAR100
with a growing number of classes. New classes are batched in group of 2, 5, 10, 20,
and 50 producing respectively 50, 20, 10, 5, and 2 tasks (CIFAR100 has eponymously
100 classes).</p>

<p>At each task, the model is evaluated on all the seen classes.</p>

<p>Finally the plotted curve is not simply the accuracy, but what <a href="https://arxiv.org/abs/1611.07725" target="_blank">(Rebuffi et al, 2017)</a> named the <strong>average incremental accuracy</strong>:</p>

<blockquote>
<p>The result of the evaluation are curves of the classification accuracies after
each batch of classes. If a single number is preferable, we report the average
of these accuracies, called average incremental accuracy.</p>
</blockquote>

<p>That I understand as: &ldquo;the accuracy at task $T + X$ is actually task $T + X$ accuracy
averaged with all previous tasks accuracy&rdquo;. Which would make sense, else two models
reaching the same accuracy on task $T + X$ would be equal while one could have
had consistently a better accuracy on previous tasks.</p>

<h4 id="learning-without-forgetting">Learning without Forgetting</h4>

<p><a href="https://arxiv.org/abs/1606.09282" target="_blank">(Li and Hoiem, 2016)</a>&rsquo;s LwF does not use any
external memory. Meaning that once the model was trained on a task, the task&rsquo;s
data is completely discarded.</p>

<p>During the first task, the training is common: softmax + cross-entropy to
classify the images. However before every other tasks, we save the current model
predictions&rsquo; confidence of old classes on the new task data.
These predictions are done on completely unseen data, that do not contain any old
classes. Still the confidences provide plenty of information: as
<a href="https://arxiv.org/abs/1503.02531" target="_blank">(Hinton et al, 2015)</a> nicknamed a <strong>dark knowledge</strong>.</p>

<p>Starting from the second task, there are now two losses:</p>

<ul>
<li>The <strong>classification loss</strong> which is a cross-entropy between the new classes&rsquo;
predictions &amp; the ground-truth</li>
<li>The <strong>distillation loss</strong> which is a binary-cross-entropy between the current
predictions of the old classes &amp; its previously recorded version.</li>
</ul>

<p>The goal of the <strong>distillation loss</strong> is to force the model to match the same
confidence on old classes as it did before the current task.</p>

<p><em>Note that as <a href="https://arxiv.org/abs/1503.02531" target="_blank">(Hinton et al, 2015)</a> suggested,
they add a temperature (the logits are raised to the power of $\frac{1}{\text{temperature}}$)
to soften the probabilities.</em></p>

<h4 id="icarl-incremental-classifier-and-representation-learning">iCaRL: Incremental Classifier and Representation Learning</h4>

<p>iCaRL also use classification &amp; distillation losses. The only difference</p>

    </div>

    





    
    

    
    <div class="article-widget">
      <div class="post-nav">
  
  <div class="post-nav-item">
    <div class="meta-nav">Next</div>
    <a href="/post/miscalibration/" rel="next">How to be confident in your neural network confidence</a>
  </div>
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Previous</div>
    <a href="/post/metric-learning/" rel="prev">Lowshot learning using a metric</a>
  </div>
  
</div>

    </div>
    

    
<button id="disqus-show"
  style="background-color: rgb(20, 22, 34); color: rgb(248, 248, 242); border-style: none">
  <i class="fa fa-angle-double-right"></i>&nbsp;Comments
</button>
<section id="comments">
  <div id="disqus_thread"></div>
<script>
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "arthurdouillard" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>


<script type="text/javascript">
  var a=document.querySelector("#disqus-show")
  var b=document.querySelector("#comments")

  b.style.display = "none";

  a.addEventListener("click",
    function(){
      ""==b.style.display?(b.style.display="none", a.innerHTML="<i class=\"fa fa fa-angle-double-right\"/i>&nbsp;Comments"):(b.style.display="",a.innerHTML="<i class=\"fa fa-angle-double-down\"></i>&nbsp;Comments");
      a.style = "background-color: rgb(20, 22, 34); color: rgb(248, 248, 242); border-style: none";
      }
  );
</script>

<style>

</style>


  </div>
</article>

<footer class="site-footer">
  <div class="container">

    
    <p class="powered-by">
      <a href="/privacy/">Privacy Policy</a>
    </p>
    

    <p class="powered-by">

      &copy; 2018 &middot; 

      Powered by the
      <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
      <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close btn-large" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">Cite</h4>
      </div>
      <div>
        <pre><code class="modal-body tex"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-primary btn-outline js-copy-cite" href="#" target="_blank">
          <i class="fa fa-copy"></i> Copy
        </a>
        <a class="btn btn-primary btn-outline js-download-cite" href="#" target="_blank">
          <i class="fa fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        CommonHTML: { linebreaks: { automatic: true } },
        tex2jax: { inlineMath: [ ['$', '$'], ['\\(','\\)'] ], displayMath: [ ['$$','$$'], ['\\[', '\\]'] ], processEscapes: false },
        TeX: { noUndefined: { attributes: { mathcolor: 'red', mathbackground: '#FFEEEE', mathsize: '90%' } } },
        messageStyle: 'none'
      });
    </script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js" integrity="sha512-3P8rXCuGJdNZOnUx/03c1jOTnMn3rP63nBip5gOP2qmUh5YAdVAvFZ1E+QLZZbC1rtMrQb+mah3AfYW11RUrWA==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    <script src="/js/hugo-academic.js"></script>
    

    
    

    
    
    

    
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/instantsearch.js/2.9.0/instantsearch.min.js" integrity="sha256-cJXigylnJlxvAdfFNHeS+IiMcKWS3Rf/cy3bl9bb0ng=" crossorigin="anonymous"></script>
    <script>
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
      };

      function getTemplate(templateName) {
        return document.querySelector(`#${templateName}-template`).innerHTML;
      }

      const options = {
        appId: "NX60KRWN2S",
        apiKey: "3693810b60d96ef8fdd9bc31f09873e6",
        indexName: "Posts",
        routing: true,
        searchParameters: {
          hitsPerPage: 10
        },
        searchFunction: function(helper) {
          let searchResults = document.querySelector('#search-hits')
          if (helper.state.query === '') {
            searchResults.style.display = 'none';
            return;
          }
          helper.search();
          searchResults.style.display = 'block';
        }
      };

      const search = instantsearch(options);

      
      search.addWidget(
        instantsearch.widgets.searchBox({
          container: '#search-box',
          placeholder: "Search..."
        })
      );

      
      search.addWidget(
        instantsearch.widgets.infiniteHits({
          container: '#search-hits',
          templates: {
            empty: '<div class="search-no-results">' + "No results found" + '</div>',
            item: getTemplate('search-hit')
          },
          cssClasses: {
            showmoreButton: 'btn btn-primary btn-outline'
          }
        })
      );

      
      search.on('render', function() {
        $('.search-hit-type').each(function( index ) {
          let content_key = $( this ).text();
          if (content_key in content_type) {
            $( this ).text(content_type[content_key]);
          }
        });
      });

      
      search.start();
    </script>
    

  </body>
</html>

