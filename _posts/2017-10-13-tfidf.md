---
layout: post
title:  TF-IDF explained
tags:   [nlp]
---

**TF-IDF** stands for **T**erm-**F**requency
**I**nverse-**D**ocument-**F**requency. You can guess from the name that is made
of two statistics: TF and IDF. TF-IDF describes the importance of a word in a
document based on a corpus of several documents. TF-IDF is one of the most
popular way to attribute weight to a term.

Each statistic, TF and IDF, has several possible ranking functions; This article
will only describe a few, for more give a look at its [Wikipedia's page](https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Definition).

To illustrate the method, we will use the following documents:

Doc #1:

```
The red fox jumps above the fence. It is agile!
```

Doc #2:

```
My computer is broken, the problem is real.
```

First of all we want to remove punctuation, normalize, and split our words:

{% highlight python linenos %}
corpus = ['The red fox jumps above the fence. It is agile!',
        'My computer is broken, the problem is real.']

def preprocess(doc):
    for punct in [',', '.', '!']:
        doc = doc.replace(punct, '')
    return doc.lower().split(' ')

corpus = [preprocess(doc) for doc in corpus]
{% endhighlight %}

# 1. Term-Frequency

The term frequency, of one term in one document, is the ratio of occurence of
the term over the total number of words in the document:

$$tf(term, doc) = \frac{nb\_term}{nb\_total}$$

The more frequent the term is, the higher will be its TF's weight.

{% highlight python linenos %}
def tf(term, doc):
    return doc.count(term) / len(doc)
{% endhighlight %}

Thus:

{% highlight python %}
>>> tf('the', corpus[0])
0.2
>>> tf('fox', corpus[0])
0.1
>>> tf('fox', corpus[1])
0.0
>>> tf('the', corpus[1])
0.125
{% endhighlight %}

# 2. Inverse-Document-Frequency

IDF mesures the importance of the word in the corpus. If a word is present in
all documents, it probably brings little information; but if only one document
contains the word, it may says a lot about what the document is about.

$$idf(term, corpus) = log \frac{nb\_docs}{nb\_doc\_with\_term}$$

If a word appears in all documents of the corpus:

$$idf(term, corpus) = log (1) = 0$$

{% highlight python linenos %}
import math

def idf(term, corpus):
    ratio = len(corpus) / sum([term in doc for doc in corpus])
    return math.log(ratio)
{% endhighlight %}

Thus:

{% highlight python %}
>>> idf('the', corpus)
0.0
>>> idf('fox', corpus)
0.6931471805599453
>>> idf('computer', corpus)
0.6931471805599453
{% endhighlight %}

Most of time, the **IDF Smooth** is prefered over IDF, its formula is:

$$idf\_smooth(term, corpus) = log(1 + \frac{nb\_docs}{nb\_doc\_with\_term})$$

{% highlight python linenos %}
def idf_smooth(term, corpus):
    ratio = len(corpus) / sum([term in doc for doc in corpus])
    return math.log(1 + ratio)
{% endhighlight %}

The IDF Smooth has two advantages:

1. Given a never-seen-before term, it won't divide by 0.
2. A term present in all documents won't be equal to 0.

# 3. TF-IDF

Now we combine the two statistics, TF and IDF, with a product:

$$tf\_idf(term, doc, corpus) = tf(term, doc) * idf\_smooth(term, corpus)$$

{% highlight python linenos %}
def tf_idf(term, doc, corpus):
    return tf(term, corpus[doc]) * idf_smooth(term, corpus)
{% endhighlight %}

With this function we can now build a matrix; Each row is a unique word, and
each column is a document of the corpus.

# 4. Application to stop-words

Stop-words are words so common that you can remove them without losing too much
information. You can have a list of english stop-words with
[NLTK](http://www.nltk.org/):

{% highlight python %}
>>> from nltk.corpus import stopwords
>>> stopwords.words('english')
['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you',
'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his',
'himself', 'she', 'her', 'hers', 'herself', 'it', 'its',
'itself', 'they', 'them' ...
{% endhighlight %}

A high TF-IDF score means that the word has some special meaning, on the other
hand a low TF-IDF score means that the word is so common that it barely brings
any information to the document.

While there are stop-words specific to the language, such as english stop-words
that are listed in NLTK, there may be also stop-words specific to a corpus.
TF-IDF can automatically find those by selecting words with the smallest
TF-IDF scores.
